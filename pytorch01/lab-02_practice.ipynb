{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Linear Regression by PyTorch\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input and output (label)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights and bias\n",
    "W = torch.zeros(1, requires_grad=True) # requires_grad=True => gonna learn automatically without writing code manually\n",
    "b = torch.zeros(1, requires_grad=True) # zeros => init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "optimizer = optim.SGD([W, b], lr=0.01) # W, b => tensors to be trained => wanna know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 W: 1.977, b: 0.052 Cost: 0.000393\n",
      "Epoch  200/1000 W: 1.982, b: 0.041 Cost: 0.000243\n",
      "Epoch  300/1000 W: 1.986, b: 0.032 Cost: 0.000150\n",
      "Epoch  400/1000 W: 1.989, b: 0.025 Cost: 0.000093\n",
      "Epoch  500/1000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch  600/1000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch  700/1000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch  800/1000 W: 1.996, b: 0.010 Cost: 0.000014\n",
      "Epoch  900/1000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 1000/1000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# training functionally\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # MSE cost function for Linear Regression\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad() # grad => init\n",
    "    cost.backward()       # backward propagation\n",
    "    optimizer.step()      # update weights\n",
    "    \n",
    "    # W goes to 2\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/1000, W: 0.000, Cost: 18.666666, Gradient:-28.000000\n",
      "Epoch:    2/1000, W: 0.280, Cost: 13.805866, Gradient:-24.080000\n",
      "Epoch:    3/1000, W: 0.521, Cost: 10.210819, Gradient:-20.708801\n",
      "Epoch:    4/1000, W: 0.728, Cost: 7.551922, Gradient:-17.809568\n",
      "Epoch:    5/1000, W: 0.906, Cost: 5.585401, Gradient:-15.316228\n",
      "Epoch:    6/1000, W: 1.059, Cost: 4.130963, Gradient:-13.171957\n",
      "Epoch:    7/1000, W: 1.191, Cost: 3.055260, Gradient:-11.327883\n",
      "Epoch:    8/1000, W: 1.304, Cost: 2.259670, Gradient:-9.741979\n",
      "Epoch:    9/1000, W: 1.402, Cost: 1.671252, Gradient:-8.378102\n",
      "Epoch:   10/1000, W: 1.485, Cost: 1.236058, Gradient:-7.205168\n",
      "Epoch:   11/1000, W: 1.557, Cost: 0.914189, Gradient:-6.196445\n",
      "Epoch:   12/1000, W: 1.619, Cost: 0.676134, Gradient:-5.328943\n",
      "Epoch:   13/1000, W: 1.673, Cost: 0.500069, Gradient:-4.582891\n",
      "Epoch:   14/1000, W: 1.718, Cost: 0.369851, Gradient:-3.941287\n",
      "Epoch:   15/1000, W: 1.758, Cost: 0.273542, Gradient:-3.389508\n",
      "Epoch:   16/1000, W: 1.792, Cost: 0.202312, Gradient:-2.914976\n",
      "Epoch:   17/1000, W: 1.821, Cost: 0.149630, Gradient:-2.506880\n",
      "Epoch:   18/1000, W: 1.846, Cost: 0.110666, Gradient:-2.155917\n",
      "Epoch:   19/1000, W: 1.868, Cost: 0.081849, Gradient:-1.854090\n",
      "Epoch:   20/1000, W: 1.886, Cost: 0.060535, Gradient:-1.594518\n",
      "Epoch:   21/1000, W: 1.902, Cost: 0.044772, Gradient:-1.371284\n",
      "Epoch:   22/1000, W: 1.916, Cost: 0.033113, Gradient:-1.179304\n",
      "Epoch:   23/1000, W: 1.928, Cost: 0.024491, Gradient:-1.014202\n",
      "Epoch:   24/1000, W: 1.938, Cost: 0.018113, Gradient:-0.872214\n",
      "Epoch:   25/1000, W: 1.946, Cost: 0.013397, Gradient:-0.750104\n",
      "Epoch:   26/1000, W: 1.954, Cost: 0.009908, Gradient:-0.645090\n",
      "Epoch:   27/1000, W: 1.960, Cost: 0.007328, Gradient:-0.554777\n",
      "Epoch:   28/1000, W: 1.966, Cost: 0.005420, Gradient:-0.477109\n",
      "Epoch:   29/1000, W: 1.971, Cost: 0.004008, Gradient:-0.410312\n",
      "Epoch:   30/1000, W: 1.975, Cost: 0.002965, Gradient:-0.352869\n",
      "Epoch:   31/1000, W: 1.978, Cost: 0.002193, Gradient:-0.303467\n",
      "Epoch:   32/1000, W: 1.981, Cost: 0.001622, Gradient:-0.260981\n",
      "Epoch:   33/1000, W: 1.984, Cost: 0.001199, Gradient:-0.224442\n",
      "Epoch:   34/1000, W: 1.986, Cost: 0.000887, Gradient:-0.193020\n",
      "Epoch:   35/1000, W: 1.988, Cost: 0.000656, Gradient:-0.165996\n",
      "Epoch:   36/1000, W: 1.990, Cost: 0.000485, Gradient:-0.142758\n",
      "Epoch:   37/1000, W: 1.991, Cost: 0.000359, Gradient:-0.122771\n",
      "Epoch:   38/1000, W: 1.992, Cost: 0.000265, Gradient:-0.105583\n",
      "Epoch:   39/1000, W: 1.994, Cost: 0.000196, Gradient:-0.090801\n",
      "Epoch:   40/1000, W: 1.994, Cost: 0.000145, Gradient:-0.078089\n",
      "Epoch:   41/1000, W: 1.995, Cost: 0.000107, Gradient:-0.067156\n",
      "Epoch:   42/1000, W: 1.996, Cost: 0.000079, Gradient:-0.057754\n",
      "Epoch:   43/1000, W: 1.996, Cost: 0.000059, Gradient:-0.049669\n",
      "Epoch:   44/1000, W: 1.997, Cost: 0.000043, Gradient:-0.042715\n",
      "Epoch:   45/1000, W: 1.997, Cost: 0.000032, Gradient:-0.036734\n",
      "Epoch:   46/1000, W: 1.998, Cost: 0.000024, Gradient:-0.031592\n",
      "Epoch:   47/1000, W: 1.998, Cost: 0.000018, Gradient:-0.027168\n",
      "Epoch:   48/1000, W: 1.998, Cost: 0.000013, Gradient:-0.023365\n",
      "Epoch:   49/1000, W: 1.999, Cost: 0.000010, Gradient:-0.020094\n",
      "Epoch:   50/1000, W: 1.999, Cost: 0.000007, Gradient:-0.017281\n",
      "Epoch:   51/1000, W: 1.999, Cost: 0.000005, Gradient:-0.014860\n",
      "Epoch:   52/1000, W: 1.999, Cost: 0.000004, Gradient:-0.012779\n",
      "Epoch:   53/1000, W: 1.999, Cost: 0.000003, Gradient:-0.010990\n",
      "Epoch:   54/1000, W: 1.999, Cost: 0.000002, Gradient:-0.009451\n",
      "Epoch:   55/1000, W: 1.999, Cost: 0.000002, Gradient:-0.008127\n",
      "Epoch:   56/1000, W: 2.000, Cost: 0.000001, Gradient:-0.006989\n",
      "Epoch:   57/1000, W: 2.000, Cost: 0.000001, Gradient:-0.006012\n",
      "Epoch:   58/1000, W: 2.000, Cost: 0.000001, Gradient:-0.005171\n",
      "Epoch:   59/1000, W: 2.000, Cost: 0.000000, Gradient:-0.004446\n",
      "Epoch:   60/1000, W: 2.000, Cost: 0.000000, Gradient:-0.003823\n",
      "Epoch:   61/1000, W: 2.000, Cost: 0.000000, Gradient:-0.003289\n",
      "Epoch:   62/1000, W: 2.000, Cost: 0.000000, Gradient:-0.002826\n",
      "Epoch:   63/1000, W: 2.000, Cost: 0.000000, Gradient:-0.002432\n",
      "Epoch:   64/1000, W: 2.000, Cost: 0.000000, Gradient:-0.002092\n",
      "Epoch:   65/1000, W: 2.000, Cost: 0.000000, Gradient:-0.001798\n",
      "Epoch:   66/1000, W: 2.000, Cost: 0.000000, Gradient:-0.001547\n",
      "Epoch:   67/1000, W: 2.000, Cost: 0.000000, Gradient:-0.001330\n",
      "Epoch:   68/1000, W: 2.000, Cost: 0.000000, Gradient:-0.001144\n",
      "Epoch:   69/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000983\n",
      "Epoch:   70/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000846\n",
      "Epoch:   71/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000728\n",
      "Epoch:   72/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000625\n",
      "Epoch:   73/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000539\n",
      "Epoch:   74/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000463\n",
      "Epoch:   75/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000399\n",
      "Epoch:   76/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000343\n",
      "Epoch:   77/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000296\n",
      "Epoch:   78/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000254\n",
      "Epoch:   79/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000218\n",
      "Epoch:   80/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000189\n",
      "Epoch:   81/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000162\n",
      "Epoch:   82/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000138\n",
      "Epoch:   83/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000118\n",
      "Epoch:   84/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000102\n",
      "Epoch:   85/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000087\n",
      "Epoch:   86/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000075\n",
      "Epoch:   87/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000065\n",
      "Epoch:   88/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000057\n",
      "Epoch:   89/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000049\n",
      "Epoch:   90/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000042\n",
      "Epoch:   91/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000035\n",
      "Epoch:   92/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000031\n",
      "Epoch:   93/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000025\n",
      "Epoch:   94/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000022\n",
      "Epoch:   95/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000018\n",
      "Epoch:   96/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000015\n",
      "Epoch:   97/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000013\n",
      "Epoch:   98/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000011\n",
      "Epoch:   99/1000, W: 2.000, Cost: 0.000000, Gradient:-0.000009\n"
     ]
    }
   ],
   "source": [
    "# training with manually\n",
    "\n",
    "W = torch.zeros(1) \n",
    "b = torch.zeros(1) # zeros => init\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    \n",
    "    # 1) 가설 => 루프 돌면서 새로운 W, b 로 재설정\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # 2) 손실 계산 => 루프 돌면서 새로운 가설식으로 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # 3) 경사 => input 지점 (x_train) 들의 손실함수 기울기 합산\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train) # calc gradient\n",
    "    if epoch < 100:\n",
    "        print(f'Epoch: {epoch:4d}/{nb_epochs}, W: {W.item():.3f}, Cost: {cost.item():.6f}, Gradient:{gradient:.6f}')\n",
    "\n",
    "    W -= lr * gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
